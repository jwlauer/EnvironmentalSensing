{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85eed3a9",
   "metadata": {},
   "source": [
    "# Calibration Examples\n",
    "The computations in this notebook illustrate how one can perform a calibration between an observed sensor output and a known physical parameter. In this analysis, the variable \"x\" represents the sensor output, and the variable \"y\" represents the value of the parameter in known phsyical units. \n",
    "\n",
    "The examples are written in the context of a calibration between the sensor output 1/R and the physical parameter electrical conductivity. The notebook assumes that data describing the relationship betwen 1/R and EC are available at a range of different EC values.\n",
    "\n",
    "The notebook is organized into three sections. First, computations are performed to find the instrument's cell constant k, where EC = k/R. This can be computed from each data point in a straightforward way, and confidence limits on the cell constant can be developed using a simple t-test.  Next, regression is used to develop a relationship between EC and 1/R.  In this method, k represents the slope of the regression line.  Finally, analysis is performed on the logarithms of EC and 1/R, ultimately producing a power-function that describes the relationship between EC and 1/R."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16392a3e-8432-4a05-adbb-2a76440d6fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#If running in a Colab notebook, start the notebook, then remove the hashtags on the following lines\n",
    "#!git clone https://github.com/jwlauer/EnvironmentalSensing.git\n",
    "#%cd EnvironmentalSensing/AnalysisCode/Calibration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594202de-9706-4189-a692-0a5d17405b57",
   "metadata": {},
   "source": [
    "### Import Libraries\n",
    "All examples require the following libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b290221",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from scipy.stats import t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e406fd",
   "metadata": {},
   "source": [
    "### Read the example dataset\n",
    "Here, we read data from an excel spreadsheet. The relevant columsn for calibration are the column 1/R, which we expect will be realted to electrical conductivity, which is described in column Kt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ec6d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "xls = pd.ExcelFile(\"Calibration_Data.xlsx\")\n",
    "sn = xls.sheet_names\n",
    "data = pd.read_excel(xls,sheet_name = sn[0], header = 0)\n",
    "display(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3173b942",
   "metadata": {},
   "source": [
    "### Create separate dataframes for x and y\n",
    "To make the code more adaptable for other situations, we now read the known physical property into a variable we call \"x\" and the sensor output into a variable we call \"y\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6203613",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data['1/R (uS)']  #These are instrument readings\n",
    "x = data['Kt']       #These are known values (e.g., concentrations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb738604",
   "metadata": {},
   "source": [
    "## Example 1:  Simple Means\n",
    "For electrical conductivity, the most frequently-used model for relating the sensor output to the physical parameter is using the simple ratio k = EC/R. This model is sufficiently simple that we can just evaluate k for each of our datapoints, then add confidence limits to this estimate.\n",
    "\n",
    "### Compute an array of k estimates and print key statistics\n",
    "Here, we use the Pandas ability to do row-by-row computations quickly. We then print out the results using Pandas built-in .mean(), .std(), and .ser() functions, which represent the mean, standard deviation, and standard error of the data, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a2391e",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_cell = x/y\n",
    "print(f'Simple mean of cell constant for all observations = {k_cell.mean():.4f}')\n",
    "print(f'Standard deviation of cell constant for all observations = {k_cell.std():.4f}')\n",
    "print(f'Standard error of cell constant for all observations = {k_cell.sem():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426a3af8",
   "metadata": {},
   "source": [
    "### Analyze the error in electrical conductivity\n",
    "Once we have a model, we are interested in computing the precision of the model.  Here, we can do this by first computing the difference between the observed physical parameter and the estimate we get from our model.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a54449e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_k_cell = k_cell.mean()\n",
    "predicted = y*mean_k_cell  #This is the predicted EC\n",
    "observed = x   #This is the observed EC\n",
    "error = predicted - observed\n",
    "print(f'Standard deviation of error = {error.std():.1f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3136a2e",
   "metadata": {},
   "source": [
    "### Compute 95% confidence limits on the error\n",
    "This is traditionally done using the well-known student's t-distribution (a typical topic for an introductory statistics course). The confidence limits on the error, $CL_E$, represent the range within which we expect the error $E$ to fall a specified fraction $\\alpha$ of the time.\n",
    "\n",
    "$CL_E = \\overline{E} \\pm t_{({1-\\alpha/2,n-1})}s_E$\n",
    "\n",
    "The $t$ statistic is accessible using the scipy.stats library, which we've already imported. The computation of confidence limits requires the specification of the confidence interval $\\alpha$ (we use 95% here), the number of degrees of freedom, which is normally the number of data points $n$ minus the number of parameters being estimated (1 in this case), a mean error $\\overline{E}$ (which we assume is zero, since error could be positive and negative), and the standard deviation of error computed from the sample, $s_E$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f412098",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(error)\n",
    "[error_lower, error_upper] = t.interval(confidence = 0.95, # used to be alpha=0.95,\n",
    "                                          df=n-1,\n",
    "                                          loc=0,\n",
    "                                          scale=error.std()\n",
    "                                       )\n",
    "print(f'Confidence Limits on Error = \\u00B1 {(error_upper - error_lower)/2:.1f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13139e17",
   "metadata": {},
   "source": [
    "You might be bothered by the fact that this much error is a relatively large fraction of the measurement if we are at the low end of the instrument's range.  A better approach could be to analyze the relative error, that is, the error at each data point divided by the measurement at that point. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d87961",
   "metadata": {},
   "outputs": [],
   "source": [
    "relative_error = (predicted - observed)/observed\n",
    "[relative_lower, relative_upper] = t.interval(confidence = 0.95, # used to be alpha=0.95,\n",
    "                                             df = n-1,\n",
    "                                             loc = 0,\n",
    "                                             scale = relative_error.std()\n",
    "                                             )\n",
    "print(f'Confidence Limits on Relative Error = \\u00B1 {(relative_upper - relative_lower)/2:.3f}')\n",
    "print(f'Confidence Limits on Percent Error = \\u00B1 {100*(relative_upper - relative_lower)/2:.1f} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b9136d",
   "metadata": {},
   "source": [
    "These results give us a broad-brush estimate of the precision of the device.\n",
    "\n",
    "### Compute 95% confidence limits on the mean\n",
    "It is sometimes also useful to compute the confidence we have in one of our model parameters.  In this case, we can use a t-test to put confidence limits on the mean cell constant. The only difference here is that the standard deviation for a mean computed from $n$ measurements, also known as the standard error, is not simply the standard deviation of the measurements--it's actually the standard deviation of the measurements in the sample, $s$, divided by the square root of the number of measurements $n$.\n",
    "\n",
    "standard error = $\\frac{s}{\\sqrt{n}}$\n",
    "\n",
    "We can now use our equation for confidence limits to place an interval around the mean cell constant, this time using the the standard error as the sample standard deviation and using the mean cell constant as the mean value. As above, we can also present this as a fraction and/or a percentage of the mean cell constant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0842437f",
   "metadata": {},
   "outputs": [],
   "source": [
    "[k_cell_lower, k_cell_upper] = t.interval(confidence = 0.95, # used to be alpha=0.95,\n",
    "                                          df=len(k_cell)-1,\n",
    "                                          loc=k_cell.mean(),\n",
    "                                          scale=k_cell.sem()\n",
    "                                          ) \n",
    "delta_k_cell = (k_cell_upper-k_cell_lower)/2\n",
    "percent_error = delta_k_cell / k_cell.mean() * 100\n",
    "print(f'Cell constant k = {k_cell.mean():.4f} \\u00B1 {delta_k_cell:.4f}')\n",
    "print(f'This implies relative error in estimate of k of = {delta_k_cell:.4f} / {k_cell.mean():.4f} = {percent_error:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f626d929",
   "metadata": {},
   "source": [
    "Note that the confidence limits on our estimate for the parameter k are MUCH narrower (i.e., confidence is much greater) than the confidence we might have in any single measurement made by the device.\n",
    "\n",
    "## Example 2: Linear Regression\n",
    "One of the major drawbacks at stopping with example 1 is that we have not visualized the data and thus are unsure whether there were any patterns in goodness of fit that depend on EC (or even whether the data really appear linear).\n",
    "\n",
    "### Visualize Raw Data\n",
    "We can start our graphical analyis by plotting our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1dd9f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1)\n",
    "plt.rc('xtick',labelsize=8)\n",
    "plt.rc('ytick',labelsize=8)\n",
    "ax.plot(x,y,\n",
    "         color = 'black', \n",
    "         markeredgewidth=0.5,\n",
    "         marker = 'o',\n",
    "         linestyle = 'none',\n",
    "         markersize = '3',\n",
    "         fillstyle='none',\n",
    "         label = 'Observed data',\n",
    "         )\n",
    "ax.set_xlabel('EC ($\\mu S/cm^2$)')\n",
    "ax.set_ylabel('1/R') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6964c3",
   "metadata": {},
   "source": [
    "In the above dataset, data were collected in triplicate (so three points per EC value), starting at a high EC value.  The solution was then diluted by approximately a factor of two for each subsequent EC value.\n",
    "\n",
    "### Perform regression and present some goodness of fit statistics\n",
    "The scipy.stats library includes methods for doing a least-squares linear regression. The function exports the slope and y-intercept of the equation as well as the correlation coefficient $R$, the p-value, and the standard error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43caef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "slope, intercept, r_value, p_value, std_err = stats.linregress(x, y)\n",
    "print(f'Linear regression gives y = {slope:.4f}x + {intercept:.4f}')\n",
    "print(f'This implies cell constant = 1/{slope:.4f} = {1/slope:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800e9c08",
   "metadata": {},
   "source": [
    "### Define function for computing  confidence interval on estimated parameter.\n",
    "\n",
    "The approach presented here is described in detail in Lavagnini and Magno, 2007, Mass Spectrometry Reviews 26, 1-18.\n",
    "\n",
    "The function uses the observed data points x and y, along with the linear y-intercept b0, slope b1, number of replicates expected in a future measurement m (often m = 1), to present the upper and lower lower confidence limits for any value of the physical parameter $\\tilde{x}_0$ (xtilde_0 in the code). Alpha represents the probability that a measurement will fall within the confidence interval between xplus and xminus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c755feca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_limits(x,y,b0,b1,m,xtilde_0,alpha):\n",
    "    n = x.size\n",
    "    ytilde = b0 + b1*x\n",
    "    xbar = x.mean()\n",
    "    s2_yx = ((y-ytilde)**2).sum()/(n-2)\n",
    "    s2_xtilde_0 = (s2_yx/b1**2)*(1/m+1/n+((xtilde_0-xbar)**2)/((x-xbar)**2).sum())\n",
    "    s_xtilde_0 = s2_xtilde_0**0.5\n",
    "    t_stat = t.ppf(1-alpha/2,df=n-2)   \n",
    "    xplus = xtilde_0 + t_stat*s_xtilde_0\n",
    "    xminus = xtilde_0 - t_stat*s_xtilde_0\n",
    "    \n",
    "    return(xminus,xtilde_0,xplus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bca6ed2",
   "metadata": {},
   "source": [
    "With the function defined, we are now able to apply it to any input values of the known physical parameter we like.  Here, we create a wide range of posible input values using numpy's np.linspace command, evaluate the upper and lower limits using our function. Finally we compute the sensor output that would correspond with each one of our input EC values (our xhat)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981f096a",
   "metadata": {},
   "outputs": [],
   "source": [
    "xhat = np.linspace(20,50000,5)\n",
    "[xminus, xhat0, xplus] = prediction_limits(x,y,intercept,slope,1,xhat,.05)\n",
    "yhat = intercept +slope*xhat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43417acb",
   "metadata": {},
   "source": [
    "### Graph results \n",
    "Now that we have our best-fit line (xhat vs. yhat) and the confidence limits (xminus vs. yhat) and (xplus vs. yhat), we can add these to our graph. Here, we also format text to describe the calibration equation that is included in the graph. Note that the inverse of this equation is what is normally used to compute a physical value from a sensor reading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d14faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_fit = ax.plot(xhat, yhat,\n",
    "    color = 'red',\n",
    "    linewidth = 1,\n",
    "    label = f'y={intercept:4.2f} + {slope:4.2f}x, r$^2$={r_value**2:4.3f}'\n",
    "    )\n",
    "\n",
    "upper_limit = ax.plot(xplus, yhat,\n",
    "    color = 'black',\n",
    "    linewidth = 0.5,\n",
    "    label = 'confidence limits on x'\n",
    "    )\n",
    "\n",
    "lower_limit = ax.plot(xminus, yhat,\n",
    "    color = 'black',\n",
    "    linewidth = 0.5,\n",
    "    )\n",
    "\n",
    "ax.legend(frameon=False, loc='upper left', prop={'size':7})#, ncol=3)\n",
    "fig.set_size_inches(3.5,4)\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76eb67e",
   "metadata": {},
   "source": [
    "## Example 3: Log-Log Regression\n",
    "The data used in the linear regression presented in Example 2 appears clustered near the lower EC range. This effect occurs because we varied EC in the experiments by dilluting by a factor of two. The non-even nature of the range between points means that linear regression is not the best approach for fitting a line to the data. Furthermore, the errors at the high end of the EC range are evidently much greater than the error at the low end.  We can handle this by repeating all of the above analysis, but instead of performing the analysis on the raw values, we perform the analysis on the natural logs of the values.\n",
    "\n",
    "### Transform raw data to log scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f807566",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.log(x)\n",
    "y = np.log(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e555a9",
   "metadata": {},
   "source": [
    "### Graph the natural logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45141edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig2, ax2 = plt.subplots(1,1)\n",
    "plt.rc('xtick',labelsize=8)\n",
    "plt.rc('ytick',labelsize=8)\n",
    "dataseries = ax2.plot(x,y,\n",
    "         color = 'black', \n",
    "         markeredgewidth=0.5,\n",
    "         marker = 'o',\n",
    "         linestyle = 'none',\n",
    "         markersize = '3',\n",
    "         fillstyle='none',\n",
    "         label = 'Observed data', \n",
    "         )\n",
    "ax2.set_xlabel('ln(EC)')\n",
    "ax2.set_ylabel('ln(1/R)')\n",
    "fig2.set_size_inches(3.5,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322a544d",
   "metadata": {},
   "source": [
    "Note now that the data appear to have a much more even spacing.  This is because we cut the concentration by a constant fraction with each dilution. Now, because the data are spaced evenly, a regression is more appropriate.\n",
    "\n",
    "### Perform the regression and print results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5228ced7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "slope, intercept, r_value, p_value, std_err = stats.linregress(x, y)\n",
    "print(f'Linear regression gives ln(y) = {slope:.4f}ln(x) + {intercept:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1eaff0",
   "metadata": {},
   "source": [
    "### Find the prediction interval for the natural logs for a wide range of potential physical values\n",
    "\n",
    "We can now use the same function as before to estimate the prediction interval within with 95% of datapoints are expected to fall. Now, the output of the function simply gives us the range within which 95% of the natural logs would be expected. In order to make the graph, we now need to provide as input natural logs, not raw numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dadedf28",
   "metadata": {},
   "outputs": [],
   "source": [
    "xhat = np.linspace(np.log(50),np.log(50000),1000)\n",
    "[xminus, xhat0, xplus] = prediction_limits(x,y,intercept,slope,1,xhat,.05)\n",
    "yhat = intercept +slope*xhat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3328838d",
   "metadata": {},
   "source": [
    "### Graph the results in log space\n",
    "We can now produce graphs just like before, but with the x- and y-scales presenting the natural logs of the physical parameter and the sensor value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4dee847",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_fit = ax2.plot(xhat, yhat,\n",
    "    color = 'red',\n",
    "    linewidth = 1,\n",
    "    label = f'ln(y)={intercept:4.3f} + {slope:4.3f}ln(x), r$^2$={r_value**2:4.3f}'\n",
    "    )\n",
    "\n",
    "upper_limit = ax2.plot(xplus, yhat,\n",
    "    color = 'black',\n",
    "    linewidth = 0.5,\n",
    "    label = 'confidence limits on x'\n",
    "    )\n",
    "\n",
    "lower_limit = ax2.plot(xminus, yhat,\n",
    "    color = 'black',\n",
    "    linewidth = 0.5,\n",
    "    )\n",
    "ax2.legend(frameon=False, loc='upper left', prop={'size':7})#, ncol=3)\n",
    "fig2.set_size_inches(3.5,4)\n",
    "fig2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f51703f",
   "metadata": {},
   "source": [
    "Now, it's clear that most of our data points fall within the prediction interval, regardless of the raw EC value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f52ac1",
   "metadata": {},
   "source": [
    "### Re-transform to real-world units\n",
    "We now have a large number of data points that all represent the natural logarithms of the data we actually care about.  We can now re-transform these to regular units by taking the anti-log of each value. For natural logarithms, this is by use of the exponential function (from the numpy library). We do this for all values, including the prediction limits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dcbc00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.exp(x)\n",
    "y = np.exp(y)\n",
    "xhat = np.exp(xhat)\n",
    "yhat = np.exp(yhat)\n",
    "xplus = np.exp(xplus)\n",
    "xminus = np.exp(xminus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af45d9f",
   "metadata": {},
   "source": [
    "### Graph the results of the log-log regression in real units\n",
    "The only thing left to do is to graph our results. The parameters of the line on the log-log regression also allow us to present the results as a power function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de27317",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig3, ax3 = plt.subplots(1,1)\n",
    "plt.rc('xtick',labelsize=8)\n",
    "plt.rc('ytick',labelsize=8)\n",
    "dataseries = ax3.plot(x,y,\n",
    "         color = 'black', \n",
    "         markeredgewidth=0.5,\n",
    "         marker = 'o',\n",
    "         linestyle = 'none',\n",
    "         markersize = '3',\n",
    "         fillstyle='none',\n",
    "         label = 'Observed data', \n",
    "         )\n",
    "ax3.set_xlabel('EC ($\\mu S/cm^2$)')\n",
    "ax3.set_ylabel('1/R')        \n",
    "\n",
    "linear_fit = ax3.plot(xhat, yhat,\n",
    "    color = 'red',\n",
    "    linewidth = 1,\n",
    "    label = ('y=%4.3f$x^{%4.3f}$, r$^2$=%4.3f' % (intercept, slope, r_value**2))\n",
    "    )\n",
    "\n",
    "upper_limit = ax3.plot(xplus, yhat,\n",
    "    color = 'black',\n",
    "    linewidth = 0.5,\n",
    "    label = 'confidence limits on x'\n",
    "    )\n",
    "\n",
    "lower_limit = ax3.plot(xminus, yhat,\n",
    "    color = 'black',\n",
    "    linewidth = 0.5,\n",
    "    )\n",
    "\n",
    "ax3.legend(frameon=False, loc='upper left', prop={'size':7})#, ncol=3)\n",
    "fig3.set_size_inches(3.5,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4fd420",
   "metadata": {},
   "source": [
    "Now, the prediction interval expands with the range of the variability of the raw datapoints in a plausible way.\n",
    "\n",
    "### Interpretation and use with a specific sensor reading\n",
    "\n",
    "The functional relationships graphed in the figure allow a measurement made by the sensor to be converted both to a best-estimate of the physical parameter in question (EC here, as represented by the red line) as well as the best estimates of the 95% confidence limits for a measurement.  For example, a given reading on the y-axis, say 1/R = 20,000, can be converted to a best estimate on the x-axis using the parameters of the regression and the prediction limits function. Here, we use the results of the log-log regression, and the result is printed to the screen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81bd40e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_reading = 20000  #this represents 1/R\n",
    "ln_sensor_reading = np.log(sensor_reading)  #this is the log of the sensor reading--represents y in regression\n",
    "ln_physical_parameter = (ln_sensor_reading - intercept) / slope  #solved as inverse of linear equation\n",
    "ln_x = np.log(x)\n",
    "ln_y = np.log(y)\n",
    "ln_results = prediction_limits(ln_x,ln_y,intercept,slope,1,ln_physical_parameter,.05)\n",
    "[lower,best,upper] = np.exp(ln_results)\n",
    "print(f'For the case where a sensor 1/R is {sensor_reading}, the corresponding range for the physical parameter is:')\n",
    "print(f'Lower Limit = {lower:.0f}, Best Estimate = {best:.0f}, Upper Limit = {upper:.0f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25bb0f6c",
   "metadata": {},
   "source": [
    "Note that the because of the log-transformation (or more precisely--the anti-log), the difference between best estimate and the lower and upper limits is no longer precisely constant. Also note that the range between the two prediction limits as a percentage of the best estimate is similar to the error we previously computed in example 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7307e486",
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_as_percent = 100*(best-lower)/best \n",
    "upper_as_percent = 100*(upper-best)/best\n",
    "print(f'Lower limit is {lower_as_percent:.1f} % below best estimate.')\n",
    "print(f'Upper limit is {upper_as_percent:.1f} % above best estimate.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000f1191",
   "metadata": {},
   "source": [
    "Given these results, we can say that the results from this calibration exercise imply that for a relatively wide range of electrical conducitivities, this sensor would provide results that are good to approximately $\\pm$ 10% of the sensor reading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feffa582-f0e2-4de4-85d9-b65dcbce5573",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
